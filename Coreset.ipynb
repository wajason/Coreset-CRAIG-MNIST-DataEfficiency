{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¡ Coreset (CRAIG) å­é›†é¸æ“‡æ¼”ç®—æ³•ç­†è¨˜\n",
        "\n",
        "CRAIG (Coreset by RelAtIve Gradients) æ–¹æ³•æ˜¯ä¸€ç¨®è³‡æ–™æ•ˆç‡å­¸ç¿’æŠ€è¡“ï¼Œç”¨æ–¼å¾å¤§å‹è³‡æ–™é›†ä¸­é«˜æ•ˆåœ°é¸å–ä¸€å€‹å…·å‚™ä»£è¡¨æ€§çš„å­é›†ï¼ˆCoresetï¼‰ã€‚\n",
        "\n",
        "### ğŸ¯ æ¼”ç®—æ³•æ ¸å¿ƒç›®æ¨™\n",
        "\n",
        "* æ‰¾åˆ°ä¸€å€‹åŒ…å« $r$ å€‹æ¨£æœ¬çš„å­é›† $\\mathcal{S} \\subset \\mathcal{D}$ï¼Œä»¥åŠä¸€çµ„æ¬Šé‡ $\\mathbf{\\gamma}$ã€‚\n",
        "* ç›®æ¨™æ˜¯ä½¿åœ¨ **åŠ æ¬Šå­é›† $\\mathcal{S}$** ä¸Šè¨“ç·´çš„æ¨¡å‹è¡Œç‚ºï¼ˆå¦‚ç´¯ç©æ¢¯åº¦ã€æœ€çµ‚æ€§èƒ½ï¼‰èˆ‡åœ¨ **å®Œæ•´è³‡æ–™é›† $\\mathcal{D}$** ä¸Šè¨“ç·´çš„çµæœ**å„˜å¯èƒ½æ¥è¿‘**ã€‚\n",
        "\n",
        "> ğŸ“Œ **æ ¸å¿ƒé—œæ³¨é»ï¼š** Coreset é¸æ“‡åŸºæ–¼ä¿ç•™**æ¢¯åº¦è³‡è¨Š**ã€‚æ¯å€‹æ¨£æœ¬ $i$ è¢«è¡¨ç¤ºç‚ºä¸€å€‹èˆ‡æ¢¯åº¦ç›¸é—œçš„å‘é‡ $\\mathbf{g}_i$ã€‚\n",
        "\n",
        "### ğŸ“Š æ ¸å¿ƒç‰¹å¾µå‘é‡ $\\mathbf{g}$\n",
        "\n",
        "* **å®šç¾©ï¼š** $\\mathbf{g}_i$ æ˜¯æ¨¡å‹åœ¨é ç†±éšæ®µï¼ˆæˆ–éš¨æ©Ÿåˆå§‹åŒ–ï¼‰è¨ˆç®—çš„ **æœ€å¾Œä¸€å±¤çš„æ¢¯åº¦è¿‘ä¼¼å‘é‡**ã€‚\n",
        "* **è¨ˆç®—æ–¹å¼ (Cross-Entropy æå¤±è¿‘ä¼¼)ï¼š**\n",
        "    $$\\mathbf{g} = \\mathbf{p} - \\mathbf{e}_y$$\n",
        "    å…¶ä¸­ï¼š\n",
        "    * $\\mathbf{p}$ï¼šæ¨¡å‹çš„ Softmax è¼¸å‡ºæ©Ÿç‡ã€‚\n",
        "    * $\\mathbf{e}_y$ï¼šç›®æ¨™é¡åˆ¥ $y$ çš„ One-hot å‘é‡ã€‚\n",
        "\n",
        "### ğŸ› ï¸ æ¼”ç®—æ³•æ­¥é©Ÿ (Facility Location è²ªå©ªé¸æ“‡)\n",
        "\n",
        "Coreset çš„é¸æ“‡æ¡ç”¨åŸºæ–¼ Facility Location ç›®æ¨™çš„**è²ªå©ª**ç­–ç•¥ï¼š\n",
        "\n",
        "| æ­¥é©Ÿ | æè¿° | ç›®çš„ |\n",
        "| :--- | :--- | :--- |\n",
        "| **1. ç‰¹å¾µå‘é‡åŒ– ($\\mathbf{g}$)** | è¨ˆç®—æ‰€æœ‰æ¨£æœ¬çš„ $\\mathbf{g}_i$ å‘é‡ï¼ˆä¾‹å¦‚ï¼š`get_last_layer_grads_like` å‡½å¼ï¼‰ã€‚ | å°‡æ¨£æœ¬è½‰æ›ç‚ºæ¢¯åº¦ç©ºé–“ä¸­çš„é»ã€‚ |\n",
        "| **2. å­é›†å¤§å°åˆ†é…** | å°‡ç¸½ Coreset å¤§å° $r_{total}$ æ ¹æ“š**é¡åˆ¥æ¯”ä¾‹**åˆ†é…çµ¦æ¯å€‹é¡åˆ¥ $c$ï¼Œå¾—åˆ° $r_c$ã€‚ | è§£æ±ºé¡åˆ¥ä¸å¹³è¡¡ï¼Œç¢ºä¿è³‡æ–™ä»£è¡¨æ€§ã€‚ |\n",
        "| **3. Facility Location è²ªå©ªé¸æ“‡** | åœ¨ $\\mathbf{g}$ ç©ºé–“ä¸­ï¼Œå¾æ¯å€‹é¡åˆ¥ $c$ å…§**è²ªå©ª**é¸å– $r_c$ å€‹ä»£è¡¨é» $\\mathcal{S}_c$ã€‚**ç›®æ¨™æ˜¯æœ€å°åŒ–æ‰€æœ‰éé¸å®šé»åˆ°æœ€è¿‘é¸å®šé»çš„è·é›¢ç¸½å’Œ**ã€‚ | ç¢ºä¿é¸å®šçš„ Coreset é»èƒ½æœ€å¤§åŒ–åœ°**è¦†è“‹**æ•´å€‹è³‡æ–™ç©ºé–“çš„åˆ†ä½ˆï¼ˆæ¢¯åº¦ç©ºé–“ï¼‰ã€‚ |\n",
        "| **4. æ¬Šé‡åˆ†é… $\\mathbf{\\gamma}$** | å°‡å®Œæ•´è³‡æ–™é›†ä¸­çš„**æ¯å€‹åŸå§‹é»**åˆ†é…çµ¦ Coreset ä¸­**æœ€è¿‘çš„ä»£è¡¨é» $j$**ã€‚ä»£è¡¨é» $j$ çš„æ¬Šé‡ $\\gamma_j$ å³ç‚ºåˆ†é…çµ¦å®ƒçš„åŸå§‹é»æ•¸é‡ã€‚ | è³¦äºˆ Coreset é»æ­£ç¢ºçš„æ¬Šé‡ï¼Œä»¥ä¿ç•™åŸå§‹è³‡æ–™é›†çš„åˆ†ä½ˆä¿¡æ¯ã€‚ |\n",
        "\n",
        "### âœ¨ è¨“ç·´éšæ®µçš„æ‡‰ç”¨\n",
        "\n",
        "* **è³‡æ–™è¼‰å…¥å™¨ï¼š** ä½¿ç”¨ `WeightedRandomSampler`ã€‚\n",
        "* **æ¬Šé‡ï¼š** å°‡ $\\mathbf{\\gamma}$ å‘é‡ä½œç‚ºæ¡æ¨£æ¬Šé‡ã€‚\n",
        "* **æ•ˆæœï¼š** æ¬Šé‡ $\\mathbf{\\gamma}$ ç¢ºä¿äº†åœ¨è¨“ç·´æ™‚ï¼Œå½±éŸ¿åŠ›å¤§ï¼ˆä»£è¡¨æ¨£æœ¬å¤šï¼‰çš„ Coreset é»æœƒè¢«**æ›´é »ç¹åœ°æ¡æ¨£**ï¼Œå¾è€Œåœ¨æ•¸æ“šé‡å¤§å¹…æ¸›å°‘çš„æƒ…æ³ä¸‹ï¼Œä¾ç„¶èƒ½è¿‘ä¼¼å®Œæ•´è³‡æ–™é›†çš„è¨“ç·´è·¯å¾‘ã€‚\n",
        "\n",
        "### ğŸ–¥ï¸ åº•ä¸‹å¯¦æˆ°å€"
      ],
      "metadata": {
        "id": "19NxSQRr56-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up"
      ],
      "metadata": {
        "id": "l_Ynfu2Enb2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch torchvision numpy tqdm\n",
        "\n",
        "import torch, torchvision, numpy as np, time, math\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "torch.manual_seed(0); np.random.seed(0)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "PZcNNe04ncDk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data\n"
      ],
      "metadata": {
        "id": "5bdhKePtneF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_full = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "idx_all = np.random.permutation(len(train_full))[:10000]\n",
        "train_10k = Subset(train_full, idx_all)\n",
        "test_ds = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "def make_loader(ds, batch_size=128, shuffle=True, sampler=None):\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=(shuffle and sampler is None), sampler=sampler)\n"
      ],
      "metadata": {
        "id": "E0-lPOzNneS2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Define Model"
      ],
      "metadata": {
        "id": "73BytGm5nheC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()       # å°‡è¼¸å…¥çš„2Dåœ–åƒæ”¤å¹³ç‚º1Då‘é‡\n",
        "        self.fc1 = nn.Linear(28*28, 100)    # ç¬¬ä¸€å±¤å…¨é€£æ¥å±¤\n",
        "        self.act = nn.Sigmoid()         # Sigmoidæ¿€æ´»å‡½æ•¸\n",
        "        self.fc2 = nn.Linear(100, 10)      # ç¬¬äºŒå±¤å…¨é€£æ¥å±¤ï¼Œlogits\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        zL = self.fc2(self.act(self.fc1(x)))   # é€šéç¬¬äºŒå€‹ç·šæ€§å±¤ç”Ÿæˆ10ç¶­è¼¸å‡º\n",
        "        return zL                 # logits before softmaxï¼Œè¿”å›çš„zLæ˜¯Logits\n",
        "\n",
        "def accuracy(logits, y):\n",
        "    # æ‰¾å‡ºæ¯å€‹æ¨£æœ¬åˆ†æ•¸æœ€é«˜çš„ç´¢å¼•ï¼Œå°‡é æ¸¬é¡åˆ¥èˆ‡çœŸå¯¦æ¨™ç±¤yé€²è¡Œå…ƒç´ ç´šæ¯”è¼ƒã€‚\n",
        "    return (logits.argmax(dim=1) == y).float().mean().item()\n"
      ],
      "metadata": {
        "id": "u0QjyHUHniug"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute last-layer gradient-like vectors (p - y)"
      ],
      "metadata": {
        "id": "gQHB52fQnkrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "# è¨ˆç®—æ¯å€‹æ¨£æœ¬çš„æ¢¯åº¦è¿‘ä¼¼å‘é‡ g=p-e_yï¼Œä¸¦å°‡æ‰€æœ‰æ‰¹æ¬¡çš„çµæœå †ç–Š (concatenate) æˆä¸€å€‹å¤§çš„ Tenso\n",
        "def get_last_layer_grads_like(model, loader):\n",
        "    model.eval()\n",
        "    G, Y, Xidx = [], [], []\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        p = logits.softmax(dim=1)\n",
        "        # ey æ˜¯ç›®æ¨™ one-hot å‘é‡ (B, 10)\n",
        "        ey = torch.zeros_like(p).scatter_(1, yb.view(-1,1), 1.0)\n",
        "        # g æ˜¯æ¢¯åº¦è¿‘ä¼¼å‘é‡ (p - ey) (B, 10)\n",
        "        # é—œéµè¼¸å…¥ï¼Œgä»£è¡¨äº†æ¯å€‹æ¨£æœ¬çš„ã€Œè³‡è¨Šåƒ¹å€¼ã€æˆ–ã€Œå½±éŸ¿åŠ›ã€\n",
        "        g = (p - ey).cpu()              # (B, 10)\n",
        "        G.append(g); Y.append(yb.cpu());\n",
        "    # ================= TODO =======================\n",
        "    G = torch.cat(G, dim=0).numpy() # å°‡æ‰€æœ‰æ‰¹æ¬¡çš„ (B, 10) å‘é‡å †ç–Šæˆ (N, 10)ï¼Œä¸¦è½‰ç‚º NumPy æ•¸çµ„\n",
        "    Y = torch.cat(Y, dim=0).numpy() # å°‡æ‰€æœ‰æ‰¹æ¬¡çš„æ¨™ç±¤å †ç–Šæˆ (N,)ï¼Œä¸¦è½‰ç‚º NumPy æ•¸çµ„\n",
        "    # ================= END =======================\n",
        "    return G, Y\n",
        "\n",
        "# Initialize a random model and run forward once to obtain g vectors (no backprop needed)\n",
        "warm_model = MLP().to(device)\n",
        "warm_loader = make_loader(train_10k, batch_size=256, shuffle=False)\n",
        "G, Y = get_last_layer_grads_like(warm_model, warm_loader)  # G: (N,10)"
      ],
      "metadata": {
        "id": "3cFqXYbSnlT0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Facility-location greedy selection"
      ],
      "metadata": {
        "id": "dBde0yqwnmym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ç›®æ¨™æ˜¯æ‰¾åˆ°ä¸€çµ„æœ€å…·ä»£è¡¨æ€§çš„é» Sï¼Œä»¥ä¾¿å®ƒå€‘èƒ½å¾ˆå¥½åœ°â€œè¦†è“‹â€æ•´å€‹è³‡æ–™é›†åœ¨æ¢¯åº¦ç©ºé–“ä¸­çš„åˆ†ä½ˆã€‚\n",
        "\n",
        "# ç¨ç«‹åŸ·è¡Œè²ªå©ªé¸æ“‡\n",
        "# æ‰¾åˆ°rå€‹Coreseté»ï¼Œä½¿å¾—æ‰€æœ‰åŸå§‹é»åˆ°æœ€è¿‘è¨­æ–½çš„ç¸½è·é›¢æœ€å°ã€‚\n",
        "def greedy_facloc_indices(Gc, r):\n",
        "    # Gc: (Nc, d)\n",
        "    Nc = Gc.shape[0]\n",
        "    S = []\n",
        "    best = np.full(Nc, -np.inf, dtype=np.float32)  # current best similarity per point\n",
        "    # Avoid full O(N^2) distance matrix; compute in small batches to save RAM\n",
        "    # ç›¸ä¼¼åº¦å‡½å¼\n",
        "    def sim_to_point(j):\n",
        "        # s(i,j) = -||g_i - g_j||\n",
        "        diff = Gc - Gc[j:j+1]\n",
        "        return -np.sqrt((diff*diff).sum(axis=1))\n",
        "    # Initialize: choose one center (similar to k-means++: pick closest to mean)\n",
        "    mu = Gc.mean(axis=0)\n",
        "    # é¸æ“‡æœ€æ¥è¿‘æ•´å€‹é¡åˆ¥å¹³å‡å€¼muçš„é»ï¼Œä½œç‚ºåˆå§‹ä¸­å¿ƒé»j_0ã€‚\n",
        "    j0 = np.argmin(((Gc - mu)**2).sum(axis=1))\n",
        "    s = sim_to_point(j0)\n",
        "    best = np.maximum(best, s)\n",
        "    S.append(j0)\n",
        "    # åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œè©²æ¼”ç®—æ³•å¿…é ˆå¾æ‰€æœ‰æœªè¢«é¸ä¸­çš„å€™é¸é»jä¸­ï¼Œé¸å‡ºèƒ½æä¾›æœ€å¤§é‚Šéš›å¢ç›Šçš„é»ã€‚\n",
        "    while len(S) < r:\n",
        "        # For each candidate j, compute marginal gain sum(max(best, s_Â·j) - best)\n",
        "        # Use sampling approximation to accelerate: randomly sample candidates,\n",
        "        # then compute full gain only for top-k\n",
        "        cand = np.setdiff1d(np.arange(Nc), np.array(S), assume_unique=True)\n",
        "        probe = np.random.choice(cand, size=min(2000, cand.size), replace=False)\n",
        "        est_gain = []\n",
        "        for j in probe:\n",
        "            s = sim_to_point(j)\n",
        "            est_gain.append(np.sum(np.maximum(best[probe], s[probe]) - best[probe]))\n",
        "        # è¨ˆç®—å®Œæ•´å¢ç›Š (Full Gain Calculation)\n",
        "        topk_idx = np.argsort(est_gain)[-10:]\n",
        "        topk = probe[topk_idx]\n",
        "        # Compute full gain among top-k\n",
        "        gains = []\n",
        "        for j in topk:\n",
        "            s = sim_to_point(j)\n",
        "            gains.append(np.sum(np.maximum(best, s) - best))\n",
        "        # é¸å‡ºæœ€ä½³é»argmax\n",
        "        j_star = topk[np.argmax(gains)]\n",
        "        s = sim_to_point(j_star)\n",
        "        # æ›´æ–°æ¯å€‹é»åˆ°æ–°é›†åˆçš„æœ€é«˜ç›¸ä¼¼åº¦\n",
        "        best = np.maximum(best, s)\n",
        "        # å°‡æ–°é¸å‡ºçš„ Coreset é»åŠ å…¥é›†åˆ\n",
        "        S.append(j_star)\n",
        "    return np.array(S, dtype=int)\n",
        "\n",
        "# é«˜å±¤æµç¨‹èˆ‡æ¬Šé‡åˆ†é…ä»»å‹™\n",
        "def classwise_coreset(G, Y, r_total=2000):\n",
        "    # Allocate r_total based on the class proportions of the original dataset\n",
        "    # æŒ‰æ¯”ä¾‹åˆ†é… Coreset å¤§å°ï¼Œç¢ºä¿ Coreset èƒ½å¤ è™•ç†é¡åˆ¥ä¸å¹³è¡¡å•é¡Œã€‚\n",
        "    # æ‰¾å‡ºæ¯å€‹é¡åˆ¥çš„ç´¢å¼•\n",
        "    idx_by_c = [np.where(Y==c)[0] for c in range(10)]\n",
        "    # å„é¡åˆ¥æ¨£æœ¬æ•¸\n",
        "    Ns = np.array([len(idxs) for idxs in idx_by_c])\n",
        "    # æ ¹æ“šæ¯”ä¾‹è¨ˆç®—æ¯å€‹é¡åˆ¥ r_c çš„å¤§å°\n",
        "    share = np.maximum(1, np.round(r_total * Ns / Ns.sum()).astype(int))\n",
        "    S_all, assign_gamma = [], np.zeros(len(Y), dtype=int)\n",
        "    for c in range(10):\n",
        "        Gc = G[idx_by_c[c]]\n",
        "        Sc_local = greedy_facloc_indices(Gc, r=share[c])\n",
        "        Sc_global = idx_by_c[c][Sc_local]\n",
        "        S_all.append(Sc_global)\n",
        "        # Compute Î³ (number of points assigned to each representative)\n",
        "        rep = G[Sc_global]\n",
        "        # Assign each original point to its nearest selected representative\n",
        "        # Batch processing to avoid O(Nr) memory blow-up\n",
        "        for chunk in np.array_split(idx_by_c[c], max(1, len(idx_by_c[c])//2000)):\n",
        "            # è¨ˆç®—æ‰€æœ‰åŸå§‹é»åˆ°æ‰€æœ‰ä»£è¡¨é»ä¹‹é–“çš„æ­æ°è·é›¢å¹³æ–¹\n",
        "            D = ((G[chunk, None, :] - rep[None, :, :])**2).sum(axis=2) # (m, r_c)\n",
        "            # æ‰¾å‡ºæ¯å€‹åŸå§‹é»æœ€è¿‘çš„ä»£è¡¨é»çš„ç´¢å¼•\n",
        "            nearest = D.argmin(axis=1)\n",
        "            for j in nearest:\n",
        "                # éå¢è©²ä»£è¡¨é»çš„è¨ˆæ•¸å™¨ï¼ˆæ¬Šé‡Î³ï¼‰\n",
        "                assign_gamma[Sc_global[j]] += 1\n",
        "    S_all = np.concatenate(S_all)\n",
        "    gamma = assign_gamma[S_all]\n",
        "    return S_all, gamma\n",
        "\n",
        "# Example: select 2,000 samples (~20%) as coreset\n",
        "S_idx, gamma = classwise_coreset(G, Y, r_total=2000)\n",
        "len(S_idx), gamma.sum()  # gamma sum should equal 10,000 (or number of total samples)\n"
      ],
      "metadata": {
        "id": "ggm53K89nnSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53762daf-8751-4964-a092-2cf0d4bcbb86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2001, np.int64(10000))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build data loaders"
      ],
      "metadata": {
        "id": "Lq1BSbqTnpQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_10k_loader = make_loader(train_10k, batch_size=128, shuffle=True)\n",
        "\n",
        "# Coreset subset (insert Î³ into sampler weights, so that each representative\n",
        "# is sampled proportionally to its assigned number of original samples)\n",
        "coreset_ds = Subset(train_10k, S_idx)\n",
        "weights = torch.tensor(gamma, dtype=torch.float32)\n",
        "sampler = WeightedRandomSampler(weights=weights, num_samples=int(gamma.sum().item()), replacement=True)\n",
        "coreset_loader = make_loader(coreset_ds, batch_size=128, shuffle=False, sampler=sampler)\n",
        "\n",
        "test_loader = make_loader(test_ds, batch_size=512, shuffle=False)"
      ],
      "metadata": {
        "id": "SxsEEHTWnpsO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and evaluation"
      ],
      "metadata": {
        "id": "83qe5KVYnrkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_eval(model, loader, epochs=5, use_gamma_in_loss=False):\n",
        "    model = model.to(device)\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "    ce = nn.CrossEntropyLoss(reduction='none' if use_gamma_in_loss else 'mean')\n",
        "    t0 = time.time()\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            if use_gamma_in_loss and hasattr(loader, 'sampler') and isinstance(loader.sampler, WeightedRandomSampler):\n",
        "                # The sampler does not directly provide Î³;\n",
        "                # we approximate by batch frequency (average number of times each representative appears)\n",
        "                # Simplification: do not multiply by Î³, since weighted sampling already reflects Î³\n",
        "                loss = ce(logits, yb).mean()\n",
        "            else:\n",
        "                loss = ce(logits, yb).mean()\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "    t1 = time.time()\n",
        "\n",
        "    # Evaluation phase\n",
        "    model.eval()\n",
        "    accs, losses = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            losses.append(nn.CrossEntropyLoss()(logits, yb).item())\n",
        "            accs.append(accuracy(logits, yb))\n",
        "    return {'time_sec': t1 - t0, 'test_acc': np.mean(accs), 'test_loss': np.mean(losses), 'model': model}\n",
        "\n",
        "# A) Full dataset (10k)\n",
        "res_full = train_eval(MLP(), train_10k_loader, epochs=5)\n",
        "# B) Coreset only (weighted sampling by Î³), same number of epochs\n",
        "res_core = train_eval(MLP(), coreset_loader, epochs=5)\n",
        "\n",
        "res_full, res_core"
      ],
      "metadata": {
        "id": "-uXyuxyCnsGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f47eea-fc96-46c5-fabe-9134786583fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'time_sec': 5.443616151809692,\n",
              "  'test_acc': np.float64(0.8296300560235977),\n",
              "  'test_loss': np.float64(0.7284958481788635),\n",
              "  'model': MLP(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
              "    (act): Sigmoid()\n",
              "    (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
              "  )},\n",
              " {'time_sec': 5.876196384429932,\n",
              "  'test_acc': np.float64(0.8365866273641587),\n",
              "  'test_loss': np.float64(0.7168105572462082),\n",
              "  'model': MLP(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
              "    (act): Sigmoid()\n",
              "    (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
              "  )})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print results"
      ],
      "metadata": {
        "id": "9s4Xx8nanuRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[Full 10k]  time={res_full['time_sec']:.1f}s  acc={res_full['test_acc']:.4f}  loss={res_full['test_loss']:.4f}\")\n",
        "print(f\"[Coreset ]  time={res_core['time_sec']:.1f}s  acc={res_core['test_acc']:.4f}  loss={res_core['test_loss']:.4f}\")\n"
      ],
      "metadata": {
        "id": "AoZbhrVBnuvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a93757-1463-41ea-bba6-0a175f695ea9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Full 10k]  time=5.4s  acc=0.8296  loss=0.7285\n",
            "[Coreset ]  time=5.9s  acc=0.8366  loss=0.7168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”¬ Coreset é¸æ“‡å¯¦é©—çµæœåˆ†æ\n",
        "\n",
        "æœ¬å¯¦é©—æ¯”è¼ƒäº†åœ¨å®Œæ•´è³‡æ–™é›† (10,000 ç­†æ¨£æœ¬) å’ŒåŠ æ¬Š Coreset (2,000 ç­†æ¨£æœ¬, ç´„ 20%) ä¸Šè¨“ç·´ MLP æ¨¡å‹ 5 å€‹ Epoch çš„æ€§èƒ½ã€‚\n",
        "\n",
        "### 1. è¨“ç·´çµæœæ¯”è¼ƒè¡¨\n",
        "\n",
        "| æŒ‡æ¨™ | å…¨è³‡æ–™é›† (10k) | Coreset (2k, åŠ æ¬Š) | å·®ç•° | åˆ†æ |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Test Accuracy** | $0.8296$ | $\\mathbf{0.8366}$ | $\\color{green}{+0.0070}$ | **Coreset ç•¥å‹**ï¼šé¡¯ç¤º Coreset ä¸åƒ…ä¿ç•™äº†ä»£è¡¨æ€§ï¼Œå¯èƒ½é‚„éæ¿¾äº†å†—é¤˜æˆ–ä½å½±éŸ¿çš„æ•¸æ“šï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ |\n",
        "| **Test Loss** | $0.7285$ | $\\mathbf{0.7168}$ | $\\color{green}{-0.0117}$ | **Coreset ç•¥å‹**ï¼šæå¤±æ›´ä½ï¼Œé€²ä¸€æ­¥è­‰å¯¦äº† Coreset è¨“ç·´çš„æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šè¡¨ç¾æ›´å¥½ã€‚ |\n",
        "| **Time (è¨“ç·´æ™‚é–“)** | $\\mathbf{5.4s}$ | $5.9s$ | $\\color{red}{+0.5s}$ | **å…¨è³‡æ–™é›†å‹**ï¼šæ™‚é–“ç•¥é•·æ˜¯å›  Coreset æ¡ç”¨ $\\mathbf{\\gamma}$ **åŠ æ¬Šéš¨æ©Ÿæ¡æ¨£**ã€‚ç¸½æ¡æ¨£æ¬¡æ•¸ï¼ˆæ¢¯åº¦æ›´æ–°æ¬¡æ•¸ï¼‰èˆ‡å…¨è³‡æ–™é›†ç›¸ä¼¼ (ç´„ 10,000 æ¬¡)ï¼Œé¡å¤–é–‹éŠ·ä¾†è‡ªæ¡æ¨£å™¨çš„è¤‡é›œæ€§ã€‚ |\n",
        "\n",
        "### 2. çµè«–ï¼šæ•¸æ“šé«˜æ•ˆæ€§çš„é©—è­‰\n",
        "\n",
        "#### A. è¨“ç·´æ€§èƒ½æå‡ (æ•¸æ“šå“è³ªé«˜æ–¼æ•¸é‡)\n",
        "\n",
        "* **æ•¸æ“šé«˜æ•ˆæ€§æˆåŠŸ (Data-efficiency)**ï¼šåƒ…ä½¿ç”¨ **20%** çš„ç¨ç‰¹æ¨£æœ¬ï¼Œå»é”æˆäº†**æ›´é«˜**çš„æº–ç¢ºåº¦å’Œ**æ›´ä½**çš„æå¤±ã€‚\n",
        "* **åŸç†**ï¼šCoreset æ¼”ç®—æ³•ï¼ˆåŸºæ–¼ Facility Location å’Œ $\\mathbf{g} = \\mathbf{p} - \\mathbf{e}_y$ æ¢¯åº¦è¿‘ä¼¼ï¼‰æˆåŠŸåœ°å¾æ•¸æ“šé›†ä¸­ã€Œç²¾é¸ã€å‡ºäº†å°æ¨¡å‹å„ªåŒ–æœ€æœ‰å½±éŸ¿åŠ›çš„é—œéµæ¨£æœ¬ã€‚\n",
        "\n",
        "#### B. è¨“ç·´æ™‚é–“çš„æ¬Šè¡¡ (æ¡æ¨£æ¨¡æ“¬)\n",
        "\n",
        "* Coreset çš„å„ªå‹¢åœ¨æ–¼**æ¸›å°‘éœ€è¦å„²å­˜å’Œè™•ç†çš„æ•¸æ“šé‡ (2K vs 10K)** å’Œ**æé«˜æ•¸æ“šå“è³ª**ã€‚\n",
        "* åœ¨æœ¬å¯¦é©—ä¸­ï¼Œç”±æ–¼æˆ‘å€‘è¨­ç½®äº†ç›¸åŒçš„ Epoch æ•¸ï¼Œä¸” Coreset Loader ä½¿ç”¨ $\\mathbf{\\gamma}$ æ¬Šé‡**æ¨¡æ“¬**äº†å®Œæ•´è³‡æ–™é›†çš„ç¸½æ¡æ¨£æ¬¡æ•¸ï¼Œå› æ­¤ç¸½è¨“ç·´æ™‚é–“ä¿æŒåœ¨åŒä¸€é‡ç´šã€‚\n"
      ],
      "metadata": {
        "id": "aamSD1o-F4oW"
      }
    }
  ]
}